{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertV3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7vL48GjVK-N",
        "outputId": "949348b1-5eff-4ae1-a354-ba4ff03a5ccd"
      },
      "source": [
        "!pip install transformers datasets tweet-preprocessor ray[tune] hyperopt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 23.2MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/38/0c24dce24767386123d528d27109024220db0e7a04467b658d587695241a/datasets-1.1.3-py3-none-any.whl (153kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 51.8MB/s \n",
            "\u001b[?25hCollecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl\n",
            "Collecting ray[tune]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/87/44476ad712acc1f7957cbf88d307d4a0283a740487cf85d710d0211d0135/ray-1.0.1.post1-cp36-cp36m-manylinux1_x86_64.whl (23.1MB)\n",
            "\u001b[K     |████████████████████████████████| 23.1MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 41.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 183kB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 59.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Collecting opencensus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/68/4f407bc0980158001c802222fab17e946728aef13f42e5d80d39dfc9ca67/opencensus-0.7.11-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 62.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (1.0.0)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (2.0.3)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (0.9.0)\n",
            "Collecting py-spy>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/a7/ab45c9ee3c4654edda3efbd6b8e2fa4962226718a7e3e3be6e3926bf3617/py_spy-0.3.3-py2.py3-none-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 45.4MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting aioredis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/64/1b1612d0a104f21f80eb4c6e1b6075f2e6aba8e228f46f229cfd3fdac859/aioredis-1.3.1-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (1.33.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (3.13)\n",
            "Collecting colorful\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/8e/e386e248266952d24d73ed734c2f5513f34d9557032618c8910e605dfaf6/colorful-0.5.4-py2.py3-none-any.whl (201kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 58.0MB/s \n",
            "\u001b[?25hCollecting gpustat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/69/d8c849715171aeabd61af7da080fdc60948b5a396d2422f1f4672e43d008/gpustat-0.6.0.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (3.12.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (2.6.0)\n",
            "Collecting redis<3.5.0,>=3.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/05/1fc7feedc19c123e7a95cfc9e7892eb6cdd2e5df4e9e8af6384349c1cc3d/redis-3.4.1-py2.py3-none-any.whl (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.8MB/s \n",
            "\u001b[?25hCollecting aiohttp-cors\n",
            "  Downloading https://files.pythonhosted.org/packages/13/e7/e436a0c0eb5127d8b491a9b83ecd2391c6ff7dcd5548dfaec2080a2340fd/aiohttp_cors-0.7.0-py3-none-any.whl\n",
            "Collecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e6/d4b6235d776c9b33f853e603efede5aac5a34f71ca9d3877adb30492eb4e/aiohttp-3.7.3-cp36-cp36m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 46.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (7.1.2)\n",
            "Collecting tensorboardX; extra == \"tune\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 33.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate; extra == \"tune\" in /usr/local/lib/python3.6/dist-packages (from ray[tune]) (0.8.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.15.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.11.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Collecting opencensus-context==0.1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/33/990f1bd9e7ee770fc8d3c154fc24743a96f16a0e49e14e1b7540cc2fdd93/opencensus_context-0.1.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from opencensus->ray[tune]) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray[tune]) (4.6.3)\n",
            "Collecting hiredis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/7d/6acf1c8d4f2fb327ff6feec000b4c56a20628fbe966a4c7cd16c0b80343c/hiredis-1.1.0-cp36-cp36m-manylinux2010_x86_64.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.1MB/s \n",
            "\u001b[?25hCollecting async-timeout\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from gpustat->ray[tune]) (7.352.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from gpustat->ray[tune]) (5.4.8)\n",
            "Collecting blessings>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray[tune]) (50.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[tune]) (20.3.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 55.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[tune]) (3.7.4.3)\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/35/b22524d6b9cacfb4c5eff413a069bbc17c6ea628e54da5c6c989998ced5f/multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 57.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.4.2)\n",
            "Collecting contextvars; python_version >= \"3.6\" and python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (1.52.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (1.17.2)\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray[tune]) (0.4.8)\n",
            "Building wheels for collected packages: sacremoses, gpustat, idna-ssl, contextvars\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=2d8f4057604a6fe07199da31f0c62ffe0f2d6f0dcc29c7a024417d17e50ee6b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-0.6.0-cp36-none-any.whl size=12622 sha256=cd13dd64c23b386b8d7b78f78c7054fd76a64e5d6c4b100ce4c3d910b9e58d78\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/b4/d5/fb5b7f1d040f2ff20687e3bad6867d63155dbde5a7c10f4293\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3161 sha256=1c0640337c913f0f94b8c2e411628c8329b2ae2ee18a86ab387250529a63db1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=7a3ada5371c472cf847db39fa26ffa67feef4f4fec84088527dca3f6fcf060d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built sacremoses gpustat idna-ssl contextvars\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, pyarrow, xxhash, datasets, tweet-preprocessor, immutables, contextvars, opencensus-context, opencensus, py-spy, colorama, hiredis, async-timeout, aioredis, colorful, blessings, gpustat, redis, multidict, yarl, idna-ssl, aiohttp, aiohttp-cors, tensorboardX, ray\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed aiohttp-3.7.3 aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-3.0.1 blessings-1.7 colorama-0.4.4 colorful-0.5.4 contextvars-2.4 datasets-1.1.3 gpustat-0.6.0 hiredis-1.1.0 idna-ssl-1.1.0 immutables-0.14 multidict-5.1.0 opencensus-0.7.11 opencensus-context-0.1.2 py-spy-0.3.3 pyarrow-2.0.0 ray-1.0.1.post1 redis-3.4.1 sacremoses-0.0.43 tensorboardX-2.1 tokenizers-0.9.4 transformers-4.0.0 tweet-preprocessor-0.6.0 xxhash-2.0.0 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNkxJLy0VgZ9"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import wordcloud\r\n",
        "import preprocessor as p # tweet-preprocessor\r\n",
        "import nltk\r\n",
        "import re\r\n",
        "import seaborn as sns\r\n",
        "import torch\r\n",
        "\r\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\r\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\r\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\r\n",
        "from scipy.special import softmax\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "from ray import tune\r\n",
        "from ray.tune import CLIReporter\r\n",
        "from ray.tune.schedulers import ASHAScheduler\r\n",
        "from ray.tune.suggest.hyperopt import HyperOptSearch"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWcx17WuVm25",
        "outputId": "5d312435-8c4a-46bc-fa2d-fddbc2282ecd"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvhdaJPjVx6Y",
        "outputId": "75b9b82e-1b27-4816-ed6f-974b0bc1997b"
      },
      "source": [
        "# dataset_dem = pd.read_csv('/content/drive/MyDrive/democrat_tweets_v2.csv')\r\n",
        "# dataset_gop = pd.read_csv('/content/drive/MyDrive/republican_tweets_v2.csv')\r\n",
        "\r\n",
        "# dataset_dem[\"label\"] = \"Democrat\"\r\n",
        "# dataset_gop[\"label\"] = \"Republican\"\r\n",
        "\r\n",
        "# dataset_final = pd.concat([dataset_dem, dataset_gop])\r\n",
        "# dataset_final.reset_index(drop=True, inplace=True)\r\n",
        "dataset_final = pd.read_csv(\"/content/drive/MyDrive/Data Science Lab/2020_labled_political_tweets.csv.zip\")\r\n",
        "# dataset_final=dataset_final[(dataset_final[\"party\"].any()==\"D\")]\r\n",
        "\r\n",
        "dataset_final = dataset_final.iloc[0:10000]\r\n",
        "\r\n",
        "\r\n",
        "for index, row in dataset_final.iterrows():\r\n",
        "  if str(row['party']) !=\"D\":\r\n",
        "    if str(row[\"party\"])!=\"R\":\r\n",
        "      dataset_final.drop(index, inplace=True)\r\n",
        "\r\n",
        "dataset_final.head()\r\n",
        "print(len(dataset_final))\r\n",
        "  \r\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIZTUfWrYILV",
        "outputId": "11557c51-d213-4824-9c69-0a460202ea69"
      },
      "source": [
        "dataset_final.count"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.count of       Unnamed: 0                   id  ...              source  party\n",
              "0              1  1212432932746473472  ...  Twitter for iPhone      D\n",
              "1              2  1212390455729696768  ...  Twitter for iPhone      D\n",
              "2              3  1212250054788038656  ...  Twitter for iPhone      R\n",
              "3              4  1212500813593169920  ...     Twitter Web App      R\n",
              "4              6  1212239323392856064  ...  Twitter for iPhone      D\n",
              "...          ...                  ...  ...                 ...    ...\n",
              "9995        1909  1215310037020823552  ...     Twitter Web App      R\n",
              "9996        1910  1215301007829295104  ...     Twitter Web App      D\n",
              "9997        1911  1215375331084963840  ...     Twitter Web App      R\n",
              "9998        1912  1215309395451621376  ...     Twitter Web App      R\n",
              "9999        1913  1215395091843571712  ...           TweetDeck      R\n",
              "\n",
              "[9927 rows x 9 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9mc1d1vYL9U"
      },
      "source": [
        "LABEL_MAP = {\r\n",
        "    \"D\": 0,\r\n",
        "    \"R\": 1\r\n",
        "}\r\n",
        "\r\n",
        "def buildLabels(row):\r\n",
        "    return LABEL_MAP.get(row[\"party\"])\r\n",
        "\r\n",
        "# def cleanTweet(row):\r\n",
        "#   tweet = row[\"text\"]\r\n",
        "#   tweet = str(p.clean(tweet))\r\n",
        "#   tweet = re.sub(r'[^\\w\\s]', '', tweet) # punctuation\r\n",
        "#   tweet = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", tweet) # numbers\r\n",
        "#   return tweet\r\n",
        "\r\n",
        "  \r\n",
        "dataset_final[\"party\"] = dataset_final.apply(lambda row: buildLabels(row), axis=1)\r\n",
        "# dataset_final[\"clean_text\"] = dataset_final.apply(lambda row: cleanTweet(row), \r\n",
        "                                                  # axis=1)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "VMHwTZ_3YUqh",
        "outputId": "d1316b2e-a58e-4cab-8ab8-91bee75da1bf"
      },
      "source": [
        "dataset_final.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>screen_name</th>\n",
              "      <th>user_id</th>\n",
              "      <th>time</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "      <th>source</th>\n",
              "      <th>party</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1212432932746473472</td>\n",
              "      <td>RepLoriTrahan</td>\n",
              "      <td>1079802482640019456</td>\n",
              "      <td>2020-01-01T12:58:31-05:00</td>\n",
              "      <td>https://www.twitter.com/RepLoriTrahan/statuses...</td>\n",
              "      <td>I am proud of the work we’ve done over the pas...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1212390455729696768</td>\n",
              "      <td>RepDwightEvans</td>\n",
              "      <td>90639372</td>\n",
              "      <td>2020-01-01T10:09:44-05:00</td>\n",
              "      <td>https://www.twitter.com/RepDwightEvans/statuse...</td>\n",
              "      <td>2/ @MorethanmySLE – a cancer survivor and lupu...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1212250054788038656</td>\n",
              "      <td>RepThomasMassie</td>\n",
              "      <td>975200486</td>\n",
              "      <td>2020-01-01T00:51:50-05:00</td>\n",
              "      <td>https://www.twitter.com/RepThomasMassie/status...</td>\n",
              "      <td>@ceQs17 Why are our people in Iraq, and how di...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1212500813593169920</td>\n",
              "      <td>SenCoryGardner</td>\n",
              "      <td>235217558</td>\n",
              "      <td>2020-01-01T17:28:15-05:00</td>\n",
              "      <td>https://www.twitter.com/SenCoryGardner/statuse...</td>\n",
              "      <td>@EnergyGOP @BLMNational @SenatorBennet @Senate...</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>1212239323392856064</td>\n",
              "      <td>RepGraceMeng</td>\n",
              "      <td>1051127714</td>\n",
              "      <td>2020-01-01T00:09:11-05:00</td>\n",
              "      <td>https://www.twitter.com/RepGraceMeng/statuses/...</td>\n",
              "      <td>It’s 2020! As we enter a new decade, I wish ev...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                   id  ...              source  party\n",
              "0           1  1212432932746473472  ...  Twitter for iPhone      0\n",
              "1           2  1212390455729696768  ...  Twitter for iPhone      0\n",
              "2           3  1212250054788038656  ...  Twitter for iPhone      1\n",
              "3           4  1212500813593169920  ...     Twitter Web App      1\n",
              "4           6  1212239323392856064  ...  Twitter for iPhone      0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKw02K0OYWeX"
      },
      "source": [
        "dataset_clf = dataset_final[[\"text\", \"party\"]]\r\n",
        "dataset_clf.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-4g4FPrYaZI",
        "outputId": "7e4d0e38-2e7d-45a5-b8a7-0d452e1fec4f"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(dataset_clf.index.values, \r\n",
        "                                                  dataset_clf.party.values, \r\n",
        "                                                  test_size=0.20, \r\n",
        "                                                  random_state=42, \r\n",
        "                                                  stratify=dataset_clf.party.values)\r\n",
        "\r\n",
        "dataset_clf['data_type'] = ['not_set']*dataset_final.shape[0]\r\n",
        "\r\n",
        "dataset_clf.loc[X_train, 'data_type'] = 'train'\r\n",
        "dataset_clf.loc[X_val, 'data_type'] = 'test'\r\n",
        "\r\n",
        "dataset_train = dataset_clf.loc[dataset_clf.data_type == 'train']\r\n",
        "dataset_test = dataset_clf.loc[dataset_clf.data_type == 'test']\r\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1763: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "QikHInvoYgdb",
        "outputId": "b460fda1-51d6-4fa5-d757-3d6ed5fc491c"
      },
      "source": [
        "dataset_train.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>party</th>\n",
              "      <th>data_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I am proud of the work we’ve done over the pas...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2/ @MorethanmySLE – a cancer survivor and lupu...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@ceQs17 Why are our people in Iraq, and how di...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@EnergyGOP @BLMNational @SenatorBennet @Senate...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It’s 2020! As we enter a new decade, I wish ev...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  party data_type\n",
              "0  I am proud of the work we’ve done over the pas...      0     train\n",
              "1  2/ @MorethanmySLE – a cancer survivor and lupu...      0     train\n",
              "2  @ceQs17 Why are our people in Iraq, and how di...      1     train\n",
              "3  @EnergyGOP @BLMNational @SenatorBennet @Senate...      1     train\n",
              "4  It’s 2020! As we enter a new decade, I wish ev...      0     train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkYcoIDTYk1k"
      },
      "source": [
        "def get_dataloaders(data, batch_size):\r\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \r\n",
        "                                            do_lower_case=True)\r\n",
        "  # tokenize train and test data so BERT can understand it\r\n",
        "  encoded_data_train = tokenizer.batch_encode_plus(\r\n",
        "      data[data.data_type=='train'].text.values, \r\n",
        "      add_special_tokens=True, \r\n",
        "      return_attention_mask=True, \r\n",
        "      padding=True,\r\n",
        "      max_length=64, \r\n",
        "      return_tensors='pt'\r\n",
        "  )\r\n",
        "\r\n",
        "  encoded_data_test = tokenizer.batch_encode_plus(\r\n",
        "      data[data.data_type=='test'].text.values, \r\n",
        "      add_special_tokens=True, \r\n",
        "      return_attention_mask=True, \r\n",
        "      padding=True, \r\n",
        "      max_length=64, \r\n",
        "      return_tensors='pt'\r\n",
        "  )\r\n",
        "\r\n",
        "\r\n",
        "  # destructure out the input_ids, attention masks, and labels from tokenizer & encoder output\r\n",
        "  input_ids_train = encoded_data_train['input_ids']\r\n",
        "  attention_masks_train = encoded_data_train['attention_mask']\r\n",
        "  labels_train = torch.tensor(data[data.data_type=='train'].party.values)\r\n",
        "\r\n",
        "  input_ids_test = encoded_data_test['input_ids']\r\n",
        "  attention_masks_test = encoded_data_test['attention_mask']\r\n",
        "  labels_test = torch.tensor(data[data.data_type=='test'].party.values)\r\n",
        "\r\n",
        "  train_data = TensorDataset(input_ids_train, attention_masks_train, labels_train)\r\n",
        "  test_data = TensorDataset(input_ids_test, attention_masks_test, labels_test)\r\n",
        "\r\n",
        "  train_dataloader = DataLoader(train_data, \r\n",
        "                                sampler=RandomSampler(train_data), \r\n",
        "                                batch_size=batch_size)\r\n",
        "\r\n",
        "  test_dataloader = DataLoader(test_data,\r\n",
        "                              sampler=SequentialSampler(test_data),\r\n",
        "                              batch_size=batch_size)\r\n",
        "  \r\n",
        "  return train_dataloader, test_dataloader"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S7hQvsyYr6r"
      },
      "source": [
        "def auc_score(preds, labels):\r\n",
        "  soft_preds = softmax(preds, axis=1) # logit -> probability\r\n",
        "  if np.shape(preds)[1] > 2: # check for multi-class\r\n",
        "    return roc_auc_score(labels, soft_preds, multi_class='ovr')\r\n",
        "  else:\r\n",
        "    soft_preds = soft_preds[:,1]\r\n",
        "    return roc_auc_score(labels, soft_preds)\r\n",
        "\r\n",
        "def acc_score_by_class(preds, labels):\r\n",
        "  label_dict_inverse = {v: k for k, v in LABEL_MAP.items()} \r\n",
        "\r\n",
        "  preds_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "  labels_flat = labels.flatten()\r\n",
        "\r\n",
        "  for label in np.unique(labels_flat):\r\n",
        "    y_preds = preds_flat[labels_flat==label]\r\n",
        "    y_true = labels_flat[labels_flat==label]\r\n",
        "    print(f'Class: {label_dict_inverse[label]}')\r\n",
        "    print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKPXDzq8Yvsq"
      },
      "source": [
        "def evaluate(model, dataloader, device):\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  loss_val_total = 0\r\n",
        "  predictions, true_vals = [], []\r\n",
        "  \r\n",
        "  for batch in dataloader:\r\n",
        "      \r\n",
        "      # convert data to CUDA\r\n",
        "      batch = tuple(b.to(device) for b in batch)\r\n",
        "      \r\n",
        "      inputs = {\r\n",
        "          'input_ids':      batch[0],\r\n",
        "          'attention_mask': batch[1],\r\n",
        "          'labels':         batch[2],\r\n",
        "      }\r\n",
        "\r\n",
        "      with torch.no_grad():        \r\n",
        "          outputs = model(**inputs) # get predictions\r\n",
        "          \r\n",
        "      loss = outputs[0]\r\n",
        "      logits = outputs[1]\r\n",
        "      loss_val_total += loss.item()\r\n",
        "\r\n",
        "      logits = logits.detach().cpu().numpy()\r\n",
        "      label_ids = inputs['labels'].cpu().numpy()\r\n",
        "      predictions.append(logits)\r\n",
        "      true_vals.append(label_ids)\r\n",
        "  \r\n",
        "  loss_val_avg = loss_val_total/len(dataloader) \r\n",
        "  \r\n",
        "  predictions = np.concatenate(predictions, axis=0)\r\n",
        "  true_vals = np.concatenate(true_vals, axis=0)\r\n",
        "          \r\n",
        "  return loss_val_avg, predictions, true_vals"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0I6ztBLYy6B"
      },
      "source": [
        "def train_and_hyperparam_search(config,\r\n",
        "                                model_init, # function to init a clean version of the net\r\n",
        "                                data,       # data as Pandas array\r\n",
        "                                cv          # rounds of cross-validation\r\n",
        "                                ):\r\n",
        "  losses = []\r\n",
        "  aucs = []\r\n",
        "  skf = StratifiedKFold(n_splits=cv, shuffle=True)\r\n",
        "  for train_idx, test_idx in skf.split(data.text, data.party):\r\n",
        "    model = model_init()\r\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "    model.to(device)\r\n",
        "    print(f\"Device: {device}\")\r\n",
        "\r\n",
        "    optimizer = AdamW(model.parameters(),\r\n",
        "                    lr=config['lr'],\r\n",
        "                    eps=config['eps'],\r\n",
        "                    weight_decay=config['weight_decay'])\r\n",
        "    \r\n",
        "    data.loc[train_idx, 'data_type'] = 'train'\r\n",
        "    data.loc[test_idx, 'data_type'] = 'test'\r\n",
        "    \r\n",
        "    train_dataloader, test_dataloader = get_dataloaders(data,\r\n",
        "                                                        config['batch_size'])\r\n",
        "\r\n",
        "    for epoch in range(1, config['epochs']+1):\r\n",
        "      model.train() # enter training mode\r\n",
        "      loss_train_total = 0\r\n",
        "\r\n",
        "      for batch in train_dataloader:\r\n",
        "          model.zero_grad()\r\n",
        "          \r\n",
        "          # get CUDA data\r\n",
        "          batch = tuple(b.to(device) for b in batch)\r\n",
        "          \r\n",
        "          inputs = {\r\n",
        "              'input_ids':      batch[0],\r\n",
        "              'attention_mask': batch[1],\r\n",
        "              'labels':         batch[2],\r\n",
        "          }\r\n",
        "\r\n",
        "          outputs = model(**inputs) # evaluate\r\n",
        "          \r\n",
        "          # for reference, we are using cross-entropy loss here,\r\n",
        "          # as implemented in https://huggingface.co/transformers/_modules/transformers/modeling_bert.html\r\n",
        "          loss = outputs[0]\r\n",
        "          loss_train_total += loss.item()\r\n",
        "          loss.backward() # do backprop\r\n",
        "\r\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "          optimizer.step()\r\n",
        "              \r\n",
        "      \r\n",
        "      loss_train_avg = loss_train_total/len(train_dataloader)    \r\n",
        "      print(f\"Training loss for epoch {epoch}: {loss_train_avg}\")        \r\n",
        "      \r\n",
        "      val_loss, predictions, true_vals = evaluate(model, test_dataloader, device)\r\n",
        "      auc = auc_score(predictions, true_vals)\r\n",
        "\r\n",
        "      losses.append(val_loss)\r\n",
        "      aucs.append(auc)\r\n",
        "\r\n",
        "  tune.report(loss=np.mean(losses), auc=np.mean(aucs))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE0lVJcHY2So",
        "outputId": "8d670dd7-707f-4430-9a65-a571abd7bcf3"
      },
      "source": [
        "from functools import partial\r\n",
        "\r\n",
        "def model_init():\r\n",
        "   return BertForSequenceClassification.from_pretrained('bert-base-uncased',\r\n",
        "                                                        num_labels=2,\r\n",
        "                                                        output_attentions=False,\r\n",
        "                                                        output_hidden_states=False)   \r\n",
        "\r\n",
        "   \r\n",
        "config = {\r\n",
        "    \"lr\": tune.choice([5e-5,3e-5,2e-5]),\r\n",
        "    \"eps\": tune.loguniform(1e-10, 1e-7),\r\n",
        "    \"weight_decay\": tune.loguniform(1e-10, 1e-5),\r\n",
        "    \"batch_size\": tune.choice([4,8,16, 32]),\r\n",
        "    \"epochs\": tune.choice([2, 3, 4])\r\n",
        "}\r\n",
        "\r\n",
        "scheduler = ASHAScheduler(\r\n",
        "    metric=\"auc\",\r\n",
        "    mode=\"max\",\r\n",
        "    max_t=10,\r\n",
        "    grace_period=1,\r\n",
        "    reduction_factor=2\r\n",
        ")\r\n",
        "\r\n",
        "reporter = CLIReporter(metric_columns=[\"loss\", \"auc\", \"training_iteration\"])\r\n",
        "hyperopt_search = HyperOptSearch(metric=\"auc\", mode=\"max\")\r\n",
        "\r\n",
        "result = tune.run(\r\n",
        "    partial(train_and_hyperparam_search, model_init=model_init, data=dataset_clf, cv=3),\r\n",
        "    resources_per_trial={\"cpu\": 2, \"gpu\": 1},\r\n",
        "    config=config,\r\n",
        "    num_samples=8,\r\n",
        "    scheduler=scheduler,\r\n",
        "    search_alg=hyperopt_search,\r\n",
        "    progress_reporter=reporter\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-09 04:48:32,583\tWARNING experiment.py:274 -- No name detected on trainable. Using DEFAULT.\n",
            "2020-12-09 04:48:32,583\tINFO registry.py:65 -- Detected unknown callable for trainable. Converting to class.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 3.5/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2/2 CPUs, 1/1 GPUs, 0.0/7.18 GiB heap, 0.0/2.44 GiB objects (0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-09_04-48-32\n",
            "Number of trials: 1/8 (1 RUNNING)\n",
            "+------------------+----------+-------+--------------+----------+-------------+-------+----------------+\n",
            "| Trial name       | status   | loc   |   batch_size |   epochs |         eps |    lr |   weight_decay |\n",
            "|------------------+----------+-------+--------------+----------+-------------+-------+----------------|\n",
            "| DEFAULT_ca1f6b00 | RUNNING  |       |            8 |        4 | 7.01337e-09 | 5e-05 |    1.06147e-08 |\n",
            "+------------------+----------+-------+--------------+----------+-------------+-------+----------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m 2020-12-09 04:48:42.896429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Training loss for epoch 1: 0.6835487275451854\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Training loss for epoch 2: 0.6732111519949447\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Training loss for epoch 3: 0.595390086111743\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Training loss for epoch 4: 0.532020926412537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Training loss for epoch 1: 0.5478373230052527\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Training loss for epoch 2: 0.3448163273424846\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Training loss for epoch 3: 0.22661977440989373\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Training loss for epoch 4: 0.12015516539050855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Training loss for epoch 1: 0.5515082921823802\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Training loss for epoch 2: 0.3819972619741642\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Training loss for epoch 3: 0.24851169959056543\n",
            "\u001b[2m\u001b[36m(pid=1788)\u001b[0m Training loss for epoch 4: 0.15065844849599547\n",
            "Result for DEFAULT_ca1f6b00:\n",
            "  auc: 0.8542891849803201\n",
            "  date: 2020-12-09_06-35-20\n",
            "  done: false\n",
            "  experiment_id: e1952f8c45d145c1a2e657695f7a71b5\n",
            "  experiment_tag: 1_batch_size=8,epochs=4,eps=7.0134e-09,lr=5e-05,weight_decay=1.0615e-08\n",
            "  hostname: ae3a3f497c36\n",
            "  iterations_since_restore: 1\n",
            "  loss: 0.623048476894477\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1788\n",
            "  time_since_restore: 6393.073433637619\n",
            "  time_this_iter_s: 6393.073433637619\n",
            "  time_total_s: 6393.073433637619\n",
            "  timestamp: 1607495720\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: ca1f6b00\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 6.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.8542891849803201\n",
            "Resources requested: 2/2 CPUs, 1/1 GPUs, 0.0/7.18 GiB heap, 0.0/2.44 GiB objects (0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-09_04-48-32\n",
            "Number of trials: 2/8 (1 PENDING, 1 RUNNING)\n",
            "+------------------+----------+-----------------+--------------+----------+-------------+-------+----------------+----------+----------+----------------------+\n",
            "| Trial name       | status   | loc             |   batch_size |   epochs |         eps |    lr |   weight_decay |     loss |      auc |   training_iteration |\n",
            "|------------------+----------+-----------------+--------------+----------+-------------+-------+----------------+----------+----------+----------------------|\n",
            "| DEFAULT_ca1f6b00 | RUNNING  | 172.28.0.2:1788 |            8 |        4 | 7.01337e-09 | 5e-05 |    1.06147e-08 | 0.623048 | 0.854289 |                    1 |\n",
            "| DEFAULT_ca388374 | PENDING  |                 |            4 |        3 | 1.38214e-08 | 2e-05 |    3.66856e-09 |          |          |                      |\n",
            "+------------------+----------+-----------------+--------------+----------+-------------+-------+----------------+----------+----------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m 2020-12-09 06:35:22.565177: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Training loss for epoch 1: 0.5580822257597978\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Training loss for epoch 2: 0.3983444232602317\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Training loss for epoch 3: 0.19486823414855606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Training loss for epoch 1: 0.5934476337454695\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Training loss for epoch 2: 0.4362643646103755\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Training loss for epoch 3: 0.22920618985223534\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Training loss for epoch 1: 0.5700905028302954\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Training loss for epoch 2: 0.40597556474589774\n",
            "\u001b[2m\u001b[36m(pid=3273)\u001b[0m Training loss for epoch 3: 0.23785472525202098\n",
            "Result for DEFAULT_ca388374:\n",
            "  auc: 0.9150959589800137\n",
            "  date: 2020-12-09_07-59-43\n",
            "  done: false\n",
            "  experiment_id: ce834b32bfe044bb99ed75a6c9a0dcc2\n",
            "  experiment_tag: 2_batch_size=4,epochs=3,eps=1.3821e-08,lr=2e-05,weight_decay=3.6686e-09\n",
            "  hostname: ae3a3f497c36\n",
            "  iterations_since_restore: 1\n",
            "  loss: 0.6615852259183616\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3273\n",
            "  time_since_restore: 5059.164920806885\n",
            "  time_this_iter_s: 5059.164920806885\n",
            "  time_total_s: 5059.164920806885\n",
            "  timestamp: 1607500783\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: ca388374\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 6.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.8846925719801669\n",
            "Resources requested: 2/2 CPUs, 1/1 GPUs, 0.0/7.18 GiB heap, 0.0/2.44 GiB objects (0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-09_04-48-32\n",
            "Number of trials: 3/8 (1 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+------------------+------------+-----------------+--------------+----------+-------------+-------+----------------+----------+----------+----------------------+\n",
            "| Trial name       | status     | loc             |   batch_size |   epochs |         eps |    lr |   weight_decay |     loss |      auc |   training_iteration |\n",
            "|------------------+------------+-----------------+--------------+----------+-------------+-------+----------------+----------+----------+----------------------|\n",
            "| DEFAULT_ca388374 | RUNNING    | 172.28.0.2:3273 |            4 |        3 | 1.38214e-08 | 2e-05 |    3.66856e-09 | 0.661585 | 0.915096 |                    1 |\n",
            "| DEFAULT_b59c996e | PENDING    |                 |            4 |        3 | 1.73696e-10 | 3e-05 |    1.76982e-07 |          |          |                      |\n",
            "| DEFAULT_ca1f6b00 | TERMINATED |                 |            8 |        4 | 7.01337e-09 | 5e-05 |    1.06147e-08 | 0.623048 | 0.854289 |                    1 |\n",
            "+------------------+------------+-----------------+--------------+----------+-------------+-------+----------------+----------+----------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m 2020-12-09 07:59:45.301393: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Training loss for epoch 1: 0.5936496009571854\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Training loss for epoch 2: 0.44541789712528546\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Training loss for epoch 3: 0.2551010548244575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Training loss for epoch 1: 0.6074759572282928\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Training loss for epoch 2: 0.5183774960767534\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Training loss for epoch 3: 0.3645254986310023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Training loss for epoch 1: 0.5838922111486335\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Training loss for epoch 2: 0.4724805448444847\n",
            "\u001b[2m\u001b[36m(pid=4468)\u001b[0m Training loss for epoch 3: 0.29215788680777305\n",
            "Result for DEFAULT_b59c996e:\n",
            "  auc: 0.9076313046199288\n",
            "  date: 2020-12-09_09-25-34\n",
            "  done: false\n",
            "  experiment_id: efdf2a0756da4874b384ab8c8f7c0423\n",
            "  experiment_tag: 3_batch_size=4,epochs=3,eps=1.737e-10,lr=3e-05,weight_decay=1.7698e-07\n",
            "  hostname: ae3a3f497c36\n",
            "  iterations_since_restore: 1\n",
            "  loss: 0.6190323703376459\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 4468\n",
            "  time_since_restore: 5147.846544742584\n",
            "  time_this_iter_s: 5147.846544742584\n",
            "  time_total_s: 5147.846544742584\n",
            "  timestamp: 1607505934\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: b59c996e\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 6.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.9076313046199288\n",
            "Resources requested: 2/2 CPUs, 1/1 GPUs, 0.0/7.18 GiB heap, 0.0/2.44 GiB objects (0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-09_04-48-32\n",
            "Number of trials: 4/8 (1 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+------------------+------------+-----------------+--------------+----------+-------------+-------+----------------+----------+----------+----------------------+\n",
            "| Trial name       | status     | loc             |   batch_size |   epochs |         eps |    lr |   weight_decay |     loss |      auc |   training_iteration |\n",
            "|------------------+------------+-----------------+--------------+----------+-------------+-------+----------------+----------+----------+----------------------|\n",
            "| DEFAULT_b59c996e | RUNNING    | 172.28.0.2:4468 |            4 |        3 | 1.73696e-10 | 3e-05 |    1.76982e-07 | 0.619032 | 0.907631 |                    1 |\n",
            "| DEFAULT_7f40bdda | PENDING    |                 |            4 |        2 | 7.24564e-08 | 2e-05 |    3.36516e-09 |          |          |                      |\n",
            "| DEFAULT_ca1f6b00 | TERMINATED |                 |            8 |        4 | 7.01337e-09 | 5e-05 |    1.06147e-08 | 0.623048 | 0.854289 |                    1 |\n",
            "| DEFAULT_ca388374 | TERMINATED |                 |            4 |        3 | 1.38214e-08 | 2e-05 |    3.66856e-09 | 0.661585 | 0.915096 |                    1 |\n",
            "+------------------+------------+-----------------+--------------+----------+-------------+-------+----------------+----------+----------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m 2020-12-09 09:25:36.578841: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m Training loss for epoch 1: 0.5637852320126806\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m Training loss for epoch 2: 0.40993044031942205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m Training loss for epoch 1: 0.5688843014720388\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m Training loss for epoch 2: 0.417814985875271\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m Training loss for epoch 1: 0.5696041483133806\n",
            "\u001b[2m\u001b[36m(pid=5681)\u001b[0m Training loss for epoch 2: 0.41453354768581646\n",
            "Result for DEFAULT_7f40bdda:\n",
            "  auc: 0.9032861549381868\n",
            "  date: 2020-12-09_10-23-19\n",
            "  done: true\n",
            "  experiment_id: 47817298b82f45a7a1d453628503e5e8\n",
            "  experiment_tag: 4_batch_size=4,epochs=2,eps=7.2456e-08,lr=2e-05,weight_decay=3.3652e-09\n",
            "  hostname: ae3a3f497c36\n",
            "  iterations_since_restore: 1\n",
            "  loss: 0.6181415402919659\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 5681\n",
            "  time_since_restore: 3461.3867185115814\n",
            "  time_this_iter_s: 3461.3867185115814\n",
            "  time_total_s: 3461.3867185115814\n",
            "  timestamp: 1607509399\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 7f40bdda\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 6.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.9054587297790577\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.18 GiB heap, 0.0/2.44 GiB objects (0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-09_04-48-32\n",
            "Number of trials: 5/8 (1 PENDING, 4 TERMINATED)\n",
            "+------------------+------------+-------+--------------+----------+-------------+-------+----------------+----------+----------+----------------------+\n",
            "| Trial name       | status     | loc   |   batch_size |   epochs |         eps |    lr |   weight_decay |     loss |      auc |   training_iteration |\n",
            "|------------------+------------+-------+--------------+----------+-------------+-------+----------------+----------+----------+----------------------|\n",
            "| DEFAULT_7db2f0f8 | PENDING    |       |            8 |        4 | 3.92277e-08 | 2e-05 |    3.5542e-08  |          |          |                      |\n",
            "| DEFAULT_ca1f6b00 | TERMINATED |       |            8 |        4 | 7.01337e-09 | 5e-05 |    1.06147e-08 | 0.623048 | 0.854289 |                    1 |\n",
            "| DEFAULT_ca388374 | TERMINATED |       |            4 |        3 | 1.38214e-08 | 2e-05 |    3.66856e-09 | 0.661585 | 0.915096 |                    1 |\n",
            "| DEFAULT_b59c996e | TERMINATED |       |            4 |        3 | 1.73696e-10 | 3e-05 |    1.76982e-07 | 0.619032 | 0.907631 |                    1 |\n",
            "| DEFAULT_7f40bdda | TERMINATED |       |            4 |        2 | 7.24564e-08 | 2e-05 |    3.36516e-09 | 0.618142 | 0.903286 |                    1 |\n",
            "+------------------+------------+-------+--------------+----------+-------------+-------+----------------+----------+----------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m 2020-12-09 10:23:21.364842: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Training loss for epoch 1: 0.5391242097397357\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Training loss for epoch 2: 0.3259665368508395\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Training loss for epoch 3: 0.19411449234849426\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Training loss for epoch 4: 0.09768798416779896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Training loss for epoch 1: 0.55246810750037\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Training loss for epoch 2: 0.3200823299883154\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Training loss for epoch 3: 0.1721271867621302\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Training loss for epoch 4: 0.0951352877820884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Training loss for epoch 1: 0.5315990503427487\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Training loss for epoch 2: 0.32113482679211164\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Training loss for epoch 3: 0.17620900222145308\n",
            "\u001b[2m\u001b[36m(pid=6506)\u001b[0m Training loss for epoch 4: 0.1096723491039611\n",
            "Result for DEFAULT_7db2f0f8:\n",
            "  auc: 0.9190411902876336\n",
            "  date: 2020-12-09_12-09-51\n",
            "  done: false\n",
            "  experiment_id: cfcd821cb04745ca8ee649d3020ada19\n",
            "  experiment_tag: 5_batch_size=8,epochs=4,eps=3.9228e-08,lr=2e-05,weight_decay=3.5542e-08\n",
            "  hostname: ae3a3f497c36\n",
            "  iterations_since_restore: 1\n",
            "  loss: 0.5989079931771177\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 6506\n",
            "  time_since_restore: 6388.9394245147705\n",
            "  time_this_iter_s: 6388.9394245147705\n",
            "  time_total_s: 6388.9394245147705\n",
            "  timestamp: 1607515791\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 7db2f0f8\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 6.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: 0.9076313046199288\n",
            "Resources requested: 2/2 CPUs, 1/1 GPUs, 0.0/7.18 GiB heap, 0.0/2.44 GiB objects (0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-09_04-48-32\n",
            "Number of trials: 6/8 (1 PENDING, 1 RUNNING, 4 TERMINATED)\n",
            "+------------------+------------+-----------------+--------------+----------+-------------+-------+----------------+----------+----------+----------------------+\n",
            "| Trial name       | status     | loc             |   batch_size |   epochs |         eps |    lr |   weight_decay |     loss |      auc |   training_iteration |\n",
            "|------------------+------------+-----------------+--------------+----------+-------------+-------+----------------+----------+----------+----------------------|\n",
            "| DEFAULT_7db2f0f8 | RUNNING    | 172.28.0.2:6506 |            8 |        4 | 3.92277e-08 | 2e-05 |    3.5542e-08  | 0.598908 | 0.919041 |                    1 |\n",
            "| DEFAULT_8edbeb66 | PENDING    |                 |           16 |        2 | 2.46148e-08 | 2e-05 |    5.95615e-07 |          |          |                      |\n",
            "| DEFAULT_ca1f6b00 | TERMINATED |                 |            8 |        4 | 7.01337e-09 | 5e-05 |    1.06147e-08 | 0.623048 | 0.854289 |                    1 |\n",
            "| DEFAULT_ca388374 | TERMINATED |                 |            4 |        3 | 1.38214e-08 | 2e-05 |    3.66856e-09 | 0.661585 | 0.915096 |                    1 |\n",
            "| DEFAULT_b59c996e | TERMINATED |                 |            4 |        3 | 1.73696e-10 | 3e-05 |    1.76982e-07 | 0.619032 | 0.907631 |                    1 |\n",
            "| DEFAULT_7f40bdda | TERMINATED |                 |            4 |        2 | 7.24564e-08 | 2e-05 |    3.36516e-09 | 0.618142 | 0.903286 |                    1 |\n",
            "+------------------+------------+-----------------+--------------+----------+-------------+-------+----------------+----------+----------+----------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m 2020-12-09 12:09:54.108713: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m Training loss for epoch 1: 0.5418791472551904\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m Training loss for epoch 2: 0.3133268720663832\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m Training loss for epoch 1: 0.5559328304252763\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m Training loss for epoch 2: 0.3365363547992375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m Device: cuda\n",
            "\u001b[2m\u001b[36m(pid=8007)\u001b[0m Training loss for epoch 1: 0.5329610580839397\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}